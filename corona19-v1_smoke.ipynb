{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "root_path = '.../Desktop/kaggle/input/corona_challenge/'\n",
    "\n",
    "corona_features = {\"doc_id\": [None], \"source\": [None], \"title\": [None],\n",
    "                  \"abstract\": [None], \"text_body\": [None]}\n",
    "corona_df = pd.DataFrame.from_dict(corona_features)\n",
    "\n",
    "json_filenames = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most frequent unigram and bigram\n",
    "- Using topic modeling to see if we can find relevant paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_corona_df(json_filenames, df, source):\n",
    "\n",
    "    for file_name in json_filenames:\n",
    "\n",
    "        row = {\"doc_id\": None, \"source\": None, \"title\": None,\n",
    "              \"abstract\": None, \"text_body\": None}\n",
    "\n",
    "        with open(file_name) as json_data:\n",
    "            data = json.load(json_data)\n",
    "\n",
    "            doc_id = data['paper_id']\n",
    "            row['doc_id'] = doc_id\n",
    "            row['title'] = data['metadata']['title']\n",
    "\n",
    "            # Now need all of abstract. Put it all in \n",
    "            # a list then use str.join() to split it\n",
    "            # into paragraphs. \n",
    "\n",
    "            abstract_list = [abst['text'] for abst in data['abstract']]\n",
    "            abstract = \"\\n \".join(abstract_list)\n",
    "\n",
    "            row['abstract'] = abstract\n",
    "\n",
    "            # And lastly the body of the text. \n",
    "            body_list = [bt['text'] for bt in data['body_text']]\n",
    "            body = \"\\n \".join(body_list)\n",
    "            \n",
    "            row['text_body'] = body\n",
    "            \n",
    "            # Now just add to the dataframe. \n",
    "            \n",
    "            if source == 'b':\n",
    "                row['source'] = \"BIORXIV\"\n",
    "            elif source == \"c\":\n",
    "                row['source'] = \"COMMON_USE_SUB\"\n",
    "            elif source == \"n\":\n",
    "                row['source'] = \"NON_COMMON_USE\"\n",
    "            elif source == \"p\":\n",
    "                row['source'] = \"PMC_CUSTOM_LICENSE\"\n",
    "            \n",
    "            df = df.append(row, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "corona_df = return_corona_df(json_filenames, corona_df, 'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_out = corona_df.to_csv('/Users/u6066091/Desktop/kaggle/output/corona_challenge/PMC_CUSTOM_LICENSE.csv')\n",
    "root_path = '/Users/u6066091/Desktop/kaggle/output/corona_challenge/'\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob(f'{root_path}/**/*.csv', recursive=True)]\n",
    "all_4 = pd.concat([pd.read_csv(f) for f in all_filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('/Users/u6066091/Desktop/kaggle/output/corona_challenge/all_data_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = new_data.loc[0,'abstract_x']\n",
    "text = re.sub(r\"^\\[\", \"\", text)\n",
    "text = re.sub(r\"\\]$\", \"\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = [x for x in sent_tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['International aviation is growing rapidly, resulting in rising aviation greenhouse gas emissions.',\n",
       " 'Concerns about the growth trajectory of the industry and emissions have led to calls for market measures such as emissions trading and carbon levies to be introduced to restrict demand and prompt innovation.',\n",
       " \"This paper provides an overview of the science on aviation's contribution to climate change, analyses key trends in the industry since 1990, projects international civil aviation emissions to 2025 and analyses the emission intensity improvements that are necessary to offset rising international demand.\",\n",
       " 'The findings suggest international aviation carbon dioxide (CO 2 ) emissions will increase by more than 110 per cent between 2005 and 2025 (from 416 Mt to between 876 and 1013 Mt) and that it is unlikely emissions could be stabilised at levels consistent with risk averse climate targets without restricting demand.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_x</th>\n",
       "      <th>text_body</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract_y</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_full_text</th>\n",
       "      <th>full_text_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6599ebbef3d868afac9daa4f80fa075675cf03bc</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>International aviation emissions to 2025: Can ...</td>\n",
       "      <td>International aviation is growing rapidly, res...</td>\n",
       "      <td>Sixty years ago, civil aviation was an infant ...</td>\n",
       "      <td>6599ebbef3d868afac9daa4f80fa075675cf03bc</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>10.1016/j.enpol.2008.08.029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract International aviation is growing rap...</td>\n",
       "      <td>2009-01-31</td>\n",
       "      <td>Macintosh, Andrew; Wallace, Lailey</td>\n",
       "      <td>Energy Policy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>68c0bb1989b6ca2b38da32a0d992027db39f80bc</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>Spring 2020 | 1 Beijing's Hard and Soft Repres...</td>\n",
       "      <td>Hong Kong's new Police Commissioner Chris Tang...</td>\n",
       "      <td>It is also noteworthy that Tang, who was once ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6c9a692eb00e9563f550ab57838c7b29d6731e54</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>Rapid Diagnosis of Respiratory Virus Infection...</td>\n",
       "      <td>Viral respirator 5\" in(ectians represent a sig...</td>\n",
       "      <td>Diagnosis of viral infections of the respirato...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>45e7c863c8a0bf2f373a64c3d7ba1546ca26d672</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>Airborne bioaerosols and their impact on human...</td>\n",
       "      <td>Bioaerosols consist of aerosols originated bio...</td>\n",
       "      <td>Bioaerosols are very small airborne particles ...</td>\n",
       "      <td>45e7c863c8a0bf2f373a64c3d7ba1546ca26d672</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>10.1016/j.jes.2017.08.027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29778157.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract Bioaerosols consist of aerosols origi...</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>Kim, Ki-Hyun; Kabir, Ehsanul; Jahan, Shamin Ara</td>\n",
       "      <td>Journal of Environmental Sciences</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>453985e02d2419abce4d5626b093a888187cfc7a</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>Characterization of the polyadenylation activi...</td>\n",
       "      <td>Bamboo mosaic virus (BaMV) has a positive-sens...</td>\n",
       "      <td>Bamboo mosaic virus (BaMV), which is a member ...</td>\n",
       "      <td>453985e02d2419abce4d5626b093a888187cfc7a</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>10.1016/j.virol.2013.05.032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23768785.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract Bamboo mosaic virus (BaMV) has a posi...</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>Chen, I-Hsuan; Cheng, Jai-Hong; Huang, Ying-We...</td>\n",
       "      <td>Virology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                    doc_id   source  \\\n",
       "0           0  6599ebbef3d868afac9daa4f80fa075675cf03bc  BIORXIV   \n",
       "1           1  68c0bb1989b6ca2b38da32a0d992027db39f80bc  BIORXIV   \n",
       "2           2  6c9a692eb00e9563f550ab57838c7b29d6731e54  BIORXIV   \n",
       "3           3  45e7c863c8a0bf2f373a64c3d7ba1546ca26d672  BIORXIV   \n",
       "4           4  453985e02d2419abce4d5626b093a888187cfc7a  BIORXIV   \n",
       "\n",
       "                                               title  \\\n",
       "0  International aviation emissions to 2025: Can ...   \n",
       "1  Spring 2020 | 1 Beijing's Hard and Soft Repres...   \n",
       "2  Rapid Diagnosis of Respiratory Virus Infection...   \n",
       "3  Airborne bioaerosols and their impact on human...   \n",
       "4  Characterization of the polyadenylation activi...   \n",
       "\n",
       "                                          abstract_x  \\\n",
       "0  International aviation is growing rapidly, res...   \n",
       "1  Hong Kong's new Police Commissioner Chris Tang...   \n",
       "2  Viral respirator 5\" in(ectians represent a sig...   \n",
       "3  Bioaerosols consist of aerosols originated bio...   \n",
       "4  Bamboo mosaic virus (BaMV) has a positive-sens...   \n",
       "\n",
       "                                           text_body  \\\n",
       "0  Sixty years ago, civil aviation was an infant ...   \n",
       "1  It is also noteworthy that Tang, who was once ...   \n",
       "2  Diagnosis of viral infections of the respirato...   \n",
       "3  Bioaerosols are very small airborne particles ...   \n",
       "4  Bamboo mosaic virus (BaMV), which is a member ...   \n",
       "\n",
       "                                        sha  source_x  \\\n",
       "0  6599ebbef3d868afac9daa4f80fa075675cf03bc  Elsevier   \n",
       "1                                       NaN       NaN   \n",
       "2                                       NaN       NaN   \n",
       "3  45e7c863c8a0bf2f373a64c3d7ba1546ca26d672  Elsevier   \n",
       "4  453985e02d2419abce4d5626b093a888187cfc7a  Elsevier   \n",
       "\n",
       "                           doi pmcid   pubmed_id    license  \\\n",
       "0  10.1016/j.enpol.2008.08.029   NaN         NaN  els-covid   \n",
       "1                          NaN   NaN         NaN        NaN   \n",
       "2                          NaN   NaN         NaN        NaN   \n",
       "3    10.1016/j.jes.2017.08.027   NaN  29778157.0  els-covid   \n",
       "4  10.1016/j.virol.2013.05.032   NaN  23768785.0  els-covid   \n",
       "\n",
       "                                          abstract_y publish_time  \\\n",
       "0  Abstract International aviation is growing rap...   2009-01-31   \n",
       "1                                                NaN          NaN   \n",
       "2                                                NaN          NaN   \n",
       "3  Abstract Bioaerosols consist of aerosols origi...   2018-05-31   \n",
       "4  Abstract Bamboo mosaic virus (BaMV) has a posi...   2013-09-30   \n",
       "\n",
       "                                             authors  \\\n",
       "0                 Macintosh, Andrew; Wallace, Lailey   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3    Kim, Ki-Hyun; Kabir, Ehsanul; Jahan, Shamin Ara   \n",
       "4  Chen, I-Hsuan; Cheng, Jai-Hong; Huang, Ying-We...   \n",
       "\n",
       "                             journal  Microsoft Academic Paper ID  \\\n",
       "0                      Energy Policy                          NaN   \n",
       "1                                NaN                          NaN   \n",
       "2                                NaN                          NaN   \n",
       "3  Journal of Environmental Sciences                          NaN   \n",
       "4                           Virology                          NaN   \n",
       "\n",
       "  WHO #Covidence has_full_text  full_text_file  \n",
       "0            NaN          True  custom_license  \n",
       "1            NaN           NaN             NaN  \n",
       "2            NaN           NaN             NaN  \n",
       "3            NaN          True  custom_license  \n",
       "4            NaN          True  custom_license  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we know about COVID-19 risk factors?\n",
    "The hypothesis is that if a paper's abstract has smok and pulm together, it presents papers on Smoking, pre-existing pulmonary diseases as risk factors. We started with a high recall approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "risk_ind = []\n",
    "count_risk = []\n",
    "abst_risk = []\n",
    "title_cov = []\n",
    "abstract = list(new_data['abstract'])\n",
    "title = list(new_data['title'])\n",
    "cov_list = ['covid',\n",
    "                    'coronavirus disease 19',\n",
    "                    'sars cov 2', # Note that search function replaces '-' with ' '\n",
    "                    '2019 ncov',\n",
    "                    '2019ncov',\n",
    "                    r'2019 n cov\\b',\n",
    "                    r'2019n cov\\b',\n",
    "                    'ncov 2019',\n",
    "                    r'\\bn cov 2019',\n",
    "                    'coronavirus 2019',\n",
    "                    'wuhan pneumonia',\n",
    "                    'wuhan virus',\n",
    "                    'wuhan coronavirus',\n",
    "                    r'coronavirus 2\\b']\n",
    "\n",
    "matches = re.findall(keywords, text, overlapped=True)\n",
    "for cov in cov_list:\n",
    "    abst_risk.append(re.findall(cov,t) for t in title)\n",
    "for i in abstract:\n",
    "    if (str(i).lower().search('smok') != -1 or str(i).lower().find('pulm') != -1 ):\n",
    "        abst_risk.append(i)\n",
    "        count_risk.append(i.lower().count('risk')/len(i.lower()))\n",
    "corona_df_risk_smoke = new_data[new_data['abstract'].isin(abst_risk)] \n",
    "corona_df_risk_smoke['count_risk'] = count_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using h_index as a factor for ranking the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_index = pd.read_csv('/Users/u6066091/Downloads/scimagoj_2018.csv',sep = ';')\n",
    "# journal_h_index = list(corona_df_risk_covid[corona_df_risk_covid['journal'].isin(h_index['Title'])]['journal'].unique())\n",
    "df_smoke = corona_df_risk_smoke.join(h_index, lsuffix='journal', rsuffix='Title')\n",
    "df_smoke.columns = ['Unnamed: 0', 'doc_id', 'source', 'title', 'abstract', 'text_body',\n",
    "       'index', 'count_risk', 'sha', 'source_x', 'doi', 'pmcid', 'pubmed_id',\n",
    "       'license', 'publish_time', 'authors', 'journal',\n",
    "       'Microsoft Academic Paper ID', 'WHO #Covidence', 'has_full_text',\n",
    "       'full_text_file', 'Rank', 'Sourceid', 'Title', 'Type', 'Issn', 'SJR',\n",
    "       'SJR Best Quartile', 'H_index', 'Total Docs. (2018)',\n",
    "       'Total Docs. (3years)', 'Total Refs.', 'Total Cites (3years)',\n",
    "       'Citable Docs. (3years)', 'Cites / Doc. (2years)', 'Ref. / Doc.',\n",
    "       'Country', 'Publisher', 'Coverage', 'Categories']\n",
    "\n",
    "df_smoke = df_smoke[['doc_id', 'source', 'title', 'abstract', 'text_body',\n",
    "       'index', 'count_risk', 'sha', 'source_x', 'doi', 'pmcid', 'pubmed_id',\n",
    "       'license', 'publish_time', 'authors', 'journal',\n",
    "       'Microsoft Academic Paper ID', 'WHO #Covidence', 'has_full_text',\n",
    "       'full_text_file', 'Rank', 'Sourceid','Type', 'Issn', 'SJR',\n",
    "       'SJR Best Quartile', 'H_index', 'Total Docs. (2018)',\n",
    "       'Total Docs. (3years)', 'Total Refs.', 'Total Cites (3years)',\n",
    "       'Citable Docs. (3years)', 'Cites / Doc. (2years)', 'Ref. / Doc.',\n",
    "       'Country', 'Publisher', 'Coverage', 'Categories']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "risk_count = [str(i).lower().count('risk')/len(str(i).lower()) for i in df_smoke_sel['text_body']]\n",
    "covid_count = [str(i).lower().count('covi')*10/len(str(i).lower()) for i in df_smoke_sel['text_body']]\n",
    "smok_count = [str(i).lower().count('smok')/len(str(i).lower()) for i in df_smoke_sel['text_body']]\n",
    "pulm_count = [str(i).lower().count('pulm')/len(str(i).lower()) for i in df_smoke_sel['text_body']]\n",
    "\n",
    "normalized_count = np.sum([risk_count, covid_count,smok_count, pulm_count] , axis = 0)\n",
    "rank = np.array(normalized_count)*100 + np.mean(df_smoke_sel['H_index'])/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smoke['rank'] = rank\n",
    "df_smoke_sel = df_smoke.sort_values('rank', ascending = False).reset_index(drop = True).loc[0:40, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smoke_sel.to_csv('.../Desktop/kaggle/output/corona_challenge/curated/keshav/smoke_sel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphShow():\n",
    "    \"\"\"\"Create demo page\"\"\"\n",
    "    def __init__(self):\n",
    "        self.base = '''\n",
    "    <html>\n",
    "    <head>\n",
    "      <script type=\"text/javascript\" src=\"VIS/dist/vis.js\"></script>\n",
    "      <link href=\"VIS/dist/vis.css\" rel=\"stylesheet\" type=\"text/css\">\n",
    "      <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
    "    </head>\n",
    "    <body>\n",
    "    <div id=\"VIS_draw\"></div>\n",
    "    <script type=\"text/javascript\">\n",
    "      var nodes = data_nodes;\n",
    "      var edges = data_edges;\n",
    "      var container = document.getElementById(\"VIS_draw\");\n",
    "      var data = {\n",
    "        nodes: nodes,\n",
    "        edges: edges\n",
    "      };\n",
    "      var options = {\n",
    "          nodes: {\n",
    "              shape: 'circle',\n",
    "              size: 15,\n",
    "              font: {\n",
    "                  size: 15\n",
    "              }\n",
    "          },\n",
    "          edges: {\n",
    "              font: {\n",
    "                  size: 10,\n",
    "                  align: 'center'\n",
    "              },\n",
    "              color: 'red',\n",
    "              arrows: {\n",
    "                  to: {enabled: true, scaleFactor: 1.2}\n",
    "              },\n",
    "              smooth: {enabled: true}\n",
    "          },\n",
    "          physics: {\n",
    "              enabled: true\n",
    "          }\n",
    "      };\n",
    "      var network = new vis.Network(container, data, options);\n",
    "    </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    '''\n",
    "    \n",
    "\n",
    "    def create_page(self, events):\n",
    "        \"\"\"Read data\"\"\"\n",
    "        nodes = []\n",
    "        for event in events:\n",
    "            nodes.append(event[0])\n",
    "            nodes.append(event[1])\n",
    "        node_dict = {node: index for index, node in enumerate(nodes)}\n",
    "\n",
    "        data_nodes = []\n",
    "        data_edges = []\n",
    "        for node, id in node_dict.items():\n",
    "            data = {}\n",
    "            data[\"group\"] = 'Event'\n",
    "            data[\"id\"] = id\n",
    "            data[\"label\"] = node\n",
    "            data_nodes.append(data)\n",
    "\n",
    "        for edge in events:\n",
    "            data = {}\n",
    "            data['from'] = node_dict.get(edge[0])\n",
    "            data['label'] = ''\n",
    "            data['to'] = node_dict.get(edge[1])\n",
    "            data_edges.append(data)\n",
    "\n",
    "        self.create_html(data_nodes, data_edges)\n",
    "        return\n",
    "\n",
    "    def create_html(self, data_nodes, data_edges):\n",
    "        \"\"\"Generate html file\"\"\"\n",
    "        f = open('graph_show.html', 'w+')\n",
    "        html = self.base.replace('data_nodes', str(data_nodes)).replace('data_edges', str(data_edges))\n",
    "        f.write(html)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install scispacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TextrankGraph:\n",
    "    '''textrank graph'''\n",
    "    def __init__(self):\n",
    "        self.graph = defaultdict(list)\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 1000 # iteration steps\n",
    "\n",
    "    def addEdge(self, start, end, weight):\n",
    "        \"\"\"Add edge between node\"\"\"\n",
    "        self.graph[start].append((start, end, weight))\n",
    "        self.graph[end].append((end, start, weight))\n",
    "\n",
    "    def rank(self):\n",
    "        \"\"\"Rank all nodes\"\"\"\n",
    "        weight_deafault = 1.0 / (len(self.graph) or 1.0) # initialize weight\n",
    "        nodeweight_dict = defaultdict(float) # store weight of node\n",
    "        outsum_node_dict = defaultdict(float) # store wegiht of out nodes\n",
    "        for node, out_edge in self.graph.items(): # initilize nodes weight by edges\n",
    "            # node: was\n",
    "            # out_edge: [('was', 'prison', 1), ('was', 'wrong', 1), ('was', 'bad', 1)]\n",
    "            nodeweight_dict[node] = weight_deafault\n",
    "            outsum_node_dict[node] = sum((edge[2] for edge in out_edge), 0.0) # if no out edge, set weight 0\n",
    "        \n",
    "        sorted_keys = sorted(self.graph.keys()) # save node name as a list for iteration\n",
    "        step_dict = [0]\n",
    "        for step in range(1, self.steps):\n",
    "            for node in sorted_keys:\n",
    "                s = 0\n",
    "                # Node's weight calculation: \n",
    "                # (edge_weight/ node's number of out link)*node_weight[edge_node]\n",
    "                for e in self.graph[node]:\n",
    "                    s += e[2] / outsum_node_dict[e[1]] * nodeweight_dict[e[1]]\n",
    "                # Update calculation: (1-d) + d*s\n",
    "                nodeweight_dict[node] = (1 - self.d) + self.d * s\n",
    "            step_dict.append(sum(nodeweight_dict.values()))\n",
    "\n",
    "            if abs(step_dict[step] - step_dict[step - 1]) <= self.min_diff:\n",
    "                break\n",
    "\n",
    "        # min-max scale to make result in range to [0 - 1]\n",
    "        min_rank, max_rank = 0, 0 # initilize max and min wegiht value\n",
    "        for w in nodeweight_dict.values():\n",
    "            if w < min_rank:\n",
    "                min_rank = w\n",
    "            if w > max_rank:\n",
    "                max_rank = w\n",
    "\n",
    "        for n, w in nodeweight_dict.items():\n",
    "            nodeweight_dict[n] = (w - min_rank/10.0) / (max_rank - min_rank/10.0)\n",
    "\n",
    "        return nodeweight_dict\n",
    "\n",
    "\n",
    "class TextRank:\n",
    "    \"\"\"Extract keywords based on textrank graph algorithm\"\"\"\n",
    "    def __init__(self):\n",
    "        self.candi_pos = ['NOUN', 'PROPN', 'VERB'] # 名词，专有名词，动词\n",
    "        self.stop_pos = ['NUM', 'ADV'] # 数字（没有时间名词，就用数字代表了），副词\n",
    "        self.span = 5\n",
    "\n",
    "    def extract_keywords(self, word_list, num_keywords):\n",
    "        g = TextrankGraph()\n",
    "        cm = defaultdict(int)\n",
    "        for i, word in enumerate(word_list): # word_list = [['previous', 'ADJ'], ['rumor', 'NOUN']]\n",
    "            if word[1] in self.candi_pos and len(word[0]) > 1: # word = ['previous', 'ADJ']\n",
    "                for j in range(i + 1, i + self.span):\n",
    "                    if j >= len(word_list):\n",
    "                        break\n",
    "                    if word_list[j][1] not in self.candi_pos or word_list[j][1] in self.stop_pos or len(word_list[j][0]) < 2:\n",
    "                        continue\n",
    "                    pair = tuple((word[0], word_list[j][0]))\n",
    "                    cm[(pair)] +=  1\n",
    "\n",
    "        # cm = {('was', 'prison'): 1, ('become', 'prison'): 1}\n",
    "        for terms, w in cm.items():\n",
    "            g.addEdge(terms[0], terms[1], w)\n",
    "        nodes_rank = g.rank()\n",
    "        nodes_rank = sorted(nodes_rank.items(), key=lambda asd:asd[1], reverse=True)\n",
    "\n",
    "        return nodes_rank[:num_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import scispacy\n",
    "# import GraphShow\n",
    "# from textrank import TextRank\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class NewsMining():\n",
    "    \"\"\"News Mining\"\"\"\n",
    "    def __init__(self):\n",
    "        self.textranker = TextRank()\n",
    "        self.ners = ['PERSON', 'ORG', 'GPE']\n",
    "        self.ner_dict = {\n",
    "            'PERSON': 'Person',  # People, including fictional\n",
    "            'ORG': 'Organization',  # Companies, agencies, institutions, etc.\n",
    "            'GPE': 'Location',  # Countries, cities, states.\n",
    "        }\n",
    "        # dependency markers for subjects\n",
    "        self.SUBJECTS = {\"nsubj\", \"nsubjpass\",\n",
    "                         \"csubj\", \"csubjpass\", \"agent\", \"expl\"}\n",
    "        # dependency markers for objects\n",
    "        self.OBJECTS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "\n",
    "        self.graph_shower = GraphShow()\n",
    "\n",
    "    def clean_spaces(self, s):\n",
    "        s = s.replace('\\r', '')\n",
    "        s = s.replace('\\t', ' ')\n",
    "        s = s.replace('\\n', ' ')\n",
    "        return s\n",
    "\n",
    "    def remove_noisy(self, content):\n",
    "        \"\"\"Remove brackets\"\"\"\n",
    "        p1 = re.compile(r'（[^）]*）')\n",
    "        p2 = re.compile(r'\\([^\\)]*\\)')\n",
    "        return p2.sub('', p1.sub('', content))\n",
    "\n",
    "    def collect_ners(self, ents):\n",
    "        \"\"\"Collect token only with PERSON, ORG, GPE\"\"\"\n",
    "        collected_ners = []\n",
    "        for token in ents:\n",
    "            if token.label_ in self.ners:\n",
    "                collected_ners.append(token.text + '/' + token.label_)\n",
    "        return collected_ners\n",
    "\n",
    "    def conll_syntax(self, sent):\n",
    "        \"\"\"Convert one sentence to conll format.\"\"\"\n",
    "\n",
    "        tuples = list()\n",
    "        for word in sent:\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                head_idx = word.head.i + 1\n",
    "            tuples.append([word.i + 1,  # Current word index, begin with 1\n",
    "                           word.text,  # Word\n",
    "                           word.lemma_,  # Lemma\n",
    "                           word.pos_,  # Coarse-grained tag\n",
    "                           word.tag_,  # Fine-grained tag\n",
    "                           '_',\n",
    "                           head_idx,  # Head of current  Index\n",
    "                           word.dep_,  # Relation\n",
    "                           '_', '_'])\n",
    "        return tuples\n",
    "\n",
    "    def syntax_parse(self, sent):\n",
    "        \"\"\"Convert one sentence to conll format.\"\"\"\n",
    "        tuples = list()\n",
    "        for word in sent:\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                head_idx = word.head.i + 1\n",
    "            tuples.append([word.i + 1,  # Current word index, begin with 1\n",
    "                           word.text,  # Word\n",
    "                           word.pos_,  # Coarse-grained tag\n",
    "                           word.head,\n",
    "                           head_idx,  # Head of current  Index\n",
    "                           word.dep_,  # Relation\n",
    "                           ])\n",
    "        return tuples\n",
    "\n",
    "    def build_parse_chile_dict(self, sent, tuples):\n",
    "        child_dict_list = list()\n",
    "        for word in sent:\n",
    "            child_dict = dict()\n",
    "            for arc in tuples:\n",
    "                if arc[3] == word:\n",
    "                    if arc[-1] in child_dict:\n",
    "                        child_dict[arc[-1]].append(arc)\n",
    "                    else:\n",
    "                        child_dict[arc[-1]] = []\n",
    "                        child_dict[arc[-1]].append(arc)\n",
    "            child_dict_list.append([word, word.pos_, word.i, child_dict])\n",
    "        return child_dict_list\n",
    "\n",
    "    def complete_VOB(self, verb, child_dict_list):\n",
    "        '''Find VOB by SBV'''\n",
    "        for child in child_dict_list:\n",
    "            word = child[0]\n",
    "            # child_dict: {'dobj': [[7, 'startup', 'NOUN', buying, 5, 'dobj']], 'prep': [[8, 'for', 'ADP', buying, 5, 'prep']]}\n",
    "            child_dict = child[3]\n",
    "            if word == verb:\n",
    "                for object_type in self.OBJECTS:  # object_type: 'dobj'\n",
    "                    if object_type not in child_dict:\n",
    "                        continue\n",
    "                    # [7, 'startup', 'NOUN', buying, 5, 'dobj']\n",
    "                    vob = child_dict[object_type][0]\n",
    "                    obj = vob[1]  # 'startup'\n",
    "                    return obj\n",
    "        return ''\n",
    "\n",
    "    def extract_triples(self, sent):\n",
    "        svo = []\n",
    "        tuples = self.syntax_parse(sent)\n",
    "        child_dict_list = self.build_parse_chile_dict(sent, tuples)\n",
    "        for tuple in tuples:\n",
    "            rel = tuple[-1]\n",
    "            if rel in self.SUBJECTS:\n",
    "                sub_wd = tuple[1]\n",
    "                verb_wd = tuple[3]\n",
    "                obj = self.complete_VOB(verb_wd, child_dict_list)\n",
    "                subj = sub_wd\n",
    "                verb = verb_wd.text\n",
    "                if not obj:\n",
    "                    svo.append([subj, verb])\n",
    "                else:\n",
    "                    svo.append([subj, verb+' '+obj])\n",
    "        return svo\n",
    "\n",
    "    def extract_keywords(self, words_postags):\n",
    "        return self.textranker.extract_keywords(words_postags, 10)\n",
    "\n",
    "    def collect_coexist(self, ner_sents, ners):\n",
    "        \"\"\"Construct NER co-occurrence matrices\"\"\"\n",
    "        co_list = []\n",
    "        for words in ner_sents:\n",
    "            co_ners = set(ners).intersection(set(words))\n",
    "            co_info = self.combination(list(co_ners))\n",
    "            co_list += co_info\n",
    "        if not co_list:\n",
    "            return []\n",
    "        return {i[0]: i[1] for i in Counter(co_list).most_common()}\n",
    "\n",
    "    def combination(self, a):\n",
    "        '''list all combination'''\n",
    "        combines = []\n",
    "        if len(a) == 0:\n",
    "            return []\n",
    "        for i in a:\n",
    "            for j in a:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                combines.append('@'.join([i, j]))\n",
    "        return combines\n",
    "\n",
    "    def main(self, content):\n",
    "        '''Main function'''\n",
    "        if not content:\n",
    "            return []\n",
    "\n",
    "        words_postags = []  # token and its POS tag\n",
    "        ner_sents = []      # store sentences which contain NER entity\n",
    "        ners = []           # store all NER entity from whole article\n",
    "        triples = []        # store subject verb object\n",
    "        events = []         # store events\n",
    "\n",
    "        # 01 remove linebreaks and brackets\n",
    "        content = self.remove_noisy(content)\n",
    "        content = self.clean_spaces(content)\n",
    "\n",
    "        # 02 split to sentences\n",
    "        doc = nlp(content)\n",
    "\n",
    "        for i, sent in enumerate(doc.sents):\n",
    "            words_postags = [[token.text, token.pos_] for token in sent]\n",
    "            words = [token.text for token in sent]\n",
    "            postags = [token.pos_ for token in sent]\n",
    "            ents = nlp(sent.text).ents  # NER detection\n",
    "            collected_ners = self.collect_ners(ents)\n",
    "\n",
    "            if collected_ners:  # only extract triples when the sentence contains 'PERSON', 'ORG', 'GPE'\n",
    "                triple = self.extract_triples(sent)\n",
    "                if not triple:\n",
    "                    continue\n",
    "                triples += triple\n",
    "                ners += collected_ners\n",
    "                ner_sents.append(\n",
    "                    [token.text + '/' + token.label_ for token in sent.ents])\n",
    "\n",
    "        # 03 get keywords\n",
    "        keywords = [i[0] for i in self.extract_keywords(words_postags)]\n",
    "        for keyword in keywords:\n",
    "            name = keyword\n",
    "            cate = 'keyword'\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 04 add triples to event only the word in keyword\n",
    "        for t in triples:\n",
    "            if (t[0] in keywords or t[1] in keywords) and len(t[0]) > 1 and len(t[1]) > 1:\n",
    "                events.append([t[0], t[1]])\n",
    "\n",
    "        # 05 get word frequency and add to events\n",
    "        word_dict = [i for i in Counter([i[0] for i in words_postags if i[1] in [\n",
    "                                        'NOUN', 'PROPN', 'VERB'] and len(i[0]) > 1]).most_common()][:10]\n",
    "        for wd in word_dict:\n",
    "            name = wd[0]\n",
    "            cate = 'frequency'\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 06 get NER from whole article\n",
    "        ner_dict = {i[0]: i[1] for i in Counter(ners).most_common(20)}\n",
    "        for ner in ner_dict:\n",
    "            name = ner.split('/')[0]  # Jessica Miller\n",
    "            cate = self.ner_dict[ner.split('/')[1]]  # PERSON\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 07 get all NER entity co-occurrence information\n",
    "        # here ner_dict is from above 06\n",
    "        co_dict = self.collect_coexist(ner_sents, list(ner_dict.keys()))\n",
    "        co_events = [[i.split('@')[0].split(\n",
    "            '/')[0], i.split('@')[1].split('/')[0]] for i in co_dict]\n",
    "        events += co_events\n",
    "\n",
    "        # 08 show event graph\n",
    "        self.graph_shower.create_page(events)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "# from graph_show import GraphShow\n",
    "# from textrank import TextRank\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class NewsMining():\n",
    "    \"\"\"News Mining\"\"\"\n",
    "    def __init__(self):\n",
    "        self.textranker = TextRank()\n",
    "        self.ners = ['PERSON', 'ORG', 'GPE']\n",
    "        self.ner_dict = {\n",
    "            'PERSON': 'Person',  # People, including fictional\n",
    "            'ORG': 'Organization',  # Companies, agencies, institutions, etc.\n",
    "            'GPE': 'Location',  # Countries, cities, states.\n",
    "        }\n",
    "        # dependency markers for subjects\n",
    "        self.SUBJECTS = {\"nsubj\", \"nsubjpass\",\n",
    "                         \"csubj\", \"csubjpass\", \"agent\", \"expl\"}\n",
    "        # dependency markers for objects\n",
    "        self.OBJECTS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "\n",
    "        self.graph_shower = GraphShow()\n",
    "\n",
    "    def clean_spaces(self, s):\n",
    "        s = s.replace('\\r', '')\n",
    "        s = s.replace('\\t', ' ')\n",
    "        s = s.replace('\\n', ' ')\n",
    "        return s\n",
    "\n",
    "    def remove_noisy(self, content):\n",
    "        \"\"\"Remove brackets\"\"\"\n",
    "        p1 = re.compile(r'（[^）]*）')\n",
    "        p2 = re.compile(r'\\([^\\)]*\\)')\n",
    "        return p2.sub('', p1.sub('', content))\n",
    "\n",
    "    def collect_ners(self, ents):\n",
    "        \"\"\"Collect token only with PERSON, ORG, GPE\"\"\"\n",
    "        collected_ners = []\n",
    "        for token in ents:\n",
    "            if token.label_ in self.ners:\n",
    "                collected_ners.append(token.text + '/' + token.label_)\n",
    "        return collected_ners\n",
    "\n",
    "    def conll_syntax(self, sent):\n",
    "        \"\"\"Convert one sentence to conll format.\"\"\"\n",
    "\n",
    "        tuples = list()\n",
    "        for word in sent:\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                head_idx = word.head.i + 1\n",
    "            tuples.append([word.i + 1,  # Current word index, begin with 1\n",
    "                           word.text,  # Word\n",
    "                           word.lemma_,  # Lemma\n",
    "                           word.pos_,  # Coarse-grained tag\n",
    "                           word.tag_,  # Fine-grained tag\n",
    "                           '_',\n",
    "                           head_idx,  # Head of current  Index\n",
    "                           word.dep_,  # Relation\n",
    "                           '_', '_'])\n",
    "        return tuples\n",
    "\n",
    "    def syntax_parse(self, sent):\n",
    "        \"\"\"Convert one sentence to conll format.\"\"\"\n",
    "        tuples = list()\n",
    "        for word in sent:\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                head_idx = word.head.i + 1\n",
    "            tuples.append([word.i + 1,  # Current word index, begin with 1\n",
    "                           word.text,  # Word\n",
    "                           word.pos_,  # Coarse-grained tag\n",
    "                           word.head,\n",
    "                           head_idx,  # Head of current  Index\n",
    "                           word.dep_,  # Relation\n",
    "                           ])\n",
    "        return tuples\n",
    "\n",
    "    def build_parse_chile_dict(self, sent, tuples):\n",
    "        child_dict_list = list()\n",
    "        for word in sent:\n",
    "            child_dict = dict()\n",
    "            for arc in tuples:\n",
    "                if arc[3] == word:\n",
    "                    if arc[-1] in child_dict:\n",
    "                        child_dict[arc[-1]].append(arc)\n",
    "                    else:\n",
    "                        child_dict[arc[-1]] = []\n",
    "                        child_dict[arc[-1]].append(arc)\n",
    "            child_dict_list.append([word, word.pos_, word.i, child_dict])\n",
    "        return child_dict_list\n",
    "\n",
    "    def complete_VOB(self, verb, child_dict_list):\n",
    "        '''Find VOB by SBV'''\n",
    "        for child in child_dict_list:\n",
    "            word = child[0]\n",
    "            # child_dict: {'dobj': [[7, 'startup', 'NOUN', buying, 5, 'dobj']], 'prep': [[8, 'for', 'ADP', buying, 5, 'prep']]}\n",
    "            child_dict = child[3]\n",
    "            if word == verb:\n",
    "                for object_type in self.OBJECTS:  # object_type: 'dobj'\n",
    "                    if object_type not in child_dict:\n",
    "                        continue\n",
    "                    # [7, 'startup', 'NOUN', buying, 5, 'dobj']\n",
    "                    vob = child_dict[object_type][0]\n",
    "                    obj = vob[1]  # 'startup'\n",
    "                    return obj\n",
    "        return ''\n",
    "\n",
    "    def extract_triples(self, sent):\n",
    "        svo = []\n",
    "        tuples = self.syntax_parse(sent)\n",
    "        child_dict_list = self.build_parse_chile_dict(sent, tuples)\n",
    "        for tuple in tuples:\n",
    "            rel = tuple[-1]\n",
    "            if rel in self.SUBJECTS:\n",
    "                sub_wd = tuple[1]\n",
    "                verb_wd = tuple[3]\n",
    "                obj = self.complete_VOB(verb_wd, child_dict_list)\n",
    "                subj = sub_wd\n",
    "                verb = verb_wd.text\n",
    "                if not obj:\n",
    "                    svo.append([subj, verb])\n",
    "                else:\n",
    "                    svo.append([subj, verb+' '+obj])\n",
    "        return svo\n",
    "\n",
    "    def extract_keywords(self, words_postags):\n",
    "        return self.textranker.extract_keywords(words_postags, 10)\n",
    "\n",
    "    def collect_coexist(self, ner_sents, ners):\n",
    "        \"\"\"Construct NER co-occurrence matrices\"\"\"\n",
    "        co_list = []\n",
    "        for words in ner_sents:\n",
    "            co_ners = set(ners).intersection(set(words))\n",
    "            co_info = self.combination(list(co_ners))\n",
    "            co_list += co_info\n",
    "        if not co_list:\n",
    "            return []\n",
    "        return {i[0]: i[1] for i in Counter(co_list).most_common()}\n",
    "\n",
    "    def combination(self, a):\n",
    "        '''list all combination'''\n",
    "        combines = []\n",
    "        if len(a) == 0:\n",
    "            return []\n",
    "        for i in a:\n",
    "            for j in a:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                combines.append('@'.join([i, j]))\n",
    "        return combines\n",
    "\n",
    "    def main(self, content):\n",
    "        '''Main function'''\n",
    "        if not content:\n",
    "            return []\n",
    "\n",
    "        words_postags = []  # token and its POS tag\n",
    "        ner_sents = []      # store sentences which contain NER entity\n",
    "        ners = []           # store all NER entity from whole article\n",
    "        triples = []        # store subject verb object\n",
    "        events = []         # store events\n",
    "\n",
    "        # 01 remove linebreaks and brackets\n",
    "        content = self.remove_noisy(content)\n",
    "        content = self.clean_spaces(content)\n",
    "\n",
    "        # 02 split to sentences\n",
    "        doc = nlp(content)\n",
    "\n",
    "        for i, sent in enumerate(doc.sents):\n",
    "            words_postags = [[token.text, token.pos_] for token in sent]\n",
    "            words = [token.text for token in sent]\n",
    "            postags = [token.pos_ for token in sent]\n",
    "            ents = nlp(sent.text).ents  # NER detection\n",
    "            collected_ners = self.collect_ners(ents)\n",
    "\n",
    "            if collected_ners:  # only extract triples when the sentence contains 'PERSON', 'ORG', 'GPE'\n",
    "                triple = self.extract_triples(sent)\n",
    "                if not triple:\n",
    "                    continue\n",
    "                triples += triple\n",
    "                ners += collected_ners\n",
    "                ner_sents.append(\n",
    "                    [token.text + '/' + token.label_ for token in sent.ents])\n",
    "\n",
    "        # 03 get keywords\n",
    "        keywords = [i[0] for i in self.extract_keywords(words_postags)]\n",
    "        for keyword in keywords:\n",
    "            name = keyword\n",
    "            cate = 'keyword'\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 04 add triples to event only the word in keyword\n",
    "        for t in triples:\n",
    "            if (t[0] in keywords or t[1] in keywords) and len(t[0]) > 1 and len(t[1]) > 1:\n",
    "                events.append([t[0], t[1]])\n",
    "\n",
    "        # 05 get word frequency and add to events\n",
    "        word_dict = [i for i in Counter([i[0] for i in words_postags if i[1] in [\n",
    "                                        'NOUN', 'PROPN', 'VERB'] and len(i[0]) > 1]).most_common()][:10]\n",
    "        for wd in word_dict:\n",
    "            name = wd[0]\n",
    "            cate = 'frequency'\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 06 get NER from whole article\n",
    "        ner_dict = {i[0]: i[1] for i in Counter(ners).most_common(20)}\n",
    "        for ner in ner_dict:\n",
    "            name = ner.split('/')[0]  # Jessica Miller\n",
    "            cate = self.ner_dict[ner.split('/')[1]]  # PERSON\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 07 get all NER entity co-occurrence information\n",
    "        # here ner_dict is from above 06\n",
    "        co_dict = self.collect_coexist(ner_sents, list(ner_dict.keys()))\n",
    "        co_events = [[i.split('@')[0].split(\n",
    "            '/')[0], i.split('@')[1].split('/')[0]] for i in co_dict]\n",
    "        events += co_events\n",
    "\n",
    "        # 08 show event graph\n",
    "        self.graph_shower.create_page(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphShow():\n",
    "    \"\"\"\"Create demo page\"\"\"\n",
    "    def __init__(self):\n",
    "        self.base = '''\n",
    "    <html>\n",
    "    <head>\n",
    "      <script type=\"text/javascript\" src=\"VIS/dist/vis.js\"></script>\n",
    "      <link href=\"VIS/dist/vis.css\" rel=\"stylesheet\" type=\"text/css\">\n",
    "      <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
    "    </head>\n",
    "    <body>\n",
    "    <div id=\"VIS_draw\"></div>\n",
    "    <script type=\"text/javascript\">\n",
    "      var nodes = data_nodes;\n",
    "      var edges = data_edges;\n",
    "      var container = document.getElementById(\"VIS_draw\");\n",
    "      var data = {\n",
    "        nodes: nodes,\n",
    "        edges: edges\n",
    "      };\n",
    "      var options = {\n",
    "          nodes: {\n",
    "              shape: 'circle',\n",
    "              size: 15,\n",
    "              font: {\n",
    "                  size: 15\n",
    "              }\n",
    "          },\n",
    "          edges: {\n",
    "              font: {\n",
    "                  size: 10,\n",
    "                  align: 'center'\n",
    "              },\n",
    "              color: 'red',\n",
    "              arrows: {\n",
    "                  to: {enabled: true, scaleFactor: 1.2}\n",
    "              },\n",
    "              smooth: {enabled: true}\n",
    "          },\n",
    "          physics: {\n",
    "              enabled: true\n",
    "          }\n",
    "      };\n",
    "      var network = new vis.Network(container, data, options);\n",
    "    </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    '''\n",
    "    \n",
    "\n",
    "    def create_page(self, events):\n",
    "        \"\"\"Read data\"\"\"\n",
    "        nodes = []\n",
    "        for event in events:\n",
    "            nodes.append(event[0])\n",
    "            nodes.append(event[1])\n",
    "        node_dict = {node: index for index, node in enumerate(nodes)}\n",
    "\n",
    "        data_nodes = []\n",
    "        data_edges = []\n",
    "        for node, id in node_dict.items():\n",
    "            data = {}\n",
    "            data[\"group\"] = 'Event'\n",
    "            data[\"id\"] = id\n",
    "            data[\"label\"] = node\n",
    "            data_nodes.append(data)\n",
    "\n",
    "        for edge in events:\n",
    "            data = {}\n",
    "            data['from'] = node_dict.get(edge[0])\n",
    "            data['label'] = ''\n",
    "            data['to'] = node_dict.get(edge[1])\n",
    "            data_edges.append(data)\n",
    "\n",
    "        self.create_html(data_nodes, data_edges)\n",
    "        return\n",
    "\n",
    "    def create_html(self, data_nodes, data_edges):\n",
    "        \"\"\"Generate html file\"\"\"\n",
    "        f = open('graph_show.html', 'w+')\n",
    "        html = self.base.replace('data_nodes', str(data_nodes)).replace('data_edges', str(data_edges))\n",
    "        f.write(html)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = corona_df_risk_covid.at[11821,'text_body' ]\n",
    "Miner = NewsMining()\n",
    "Miner.main(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
