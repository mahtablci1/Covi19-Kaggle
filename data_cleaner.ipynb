{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "The objective of this notebook will be to clean up working dataframes and output them as either JSONs for the front end or CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df = pd.read_csv('../output/papers_df.csv', \n",
    "                        index_col=0, \n",
    "                        keep_default_na=False,\n",
    "                       parse_dates=['publish_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Functions and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloads = '../../../Downloads/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_keywords = ['COVID-19', 'HCoV-19', 'CORD-19' ,'2019-nCoV', 'Wuhan coronavirus', 'SARS-CoV-2', 'covid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_papers = papers_df[papers_df['text_body'].apply(lambda x: \n",
    "                                                     any(key.lower() in x.lower()\n",
    "                                                        for key in covid_keywords))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_metadata(df):\n",
    "    df['is_covid_related'] = df['doc_id'].apply(lambda x: x.lower() in covid_papers['doc_id'].values)\n",
    "    df = df.merge(papers_df, how='left', on='doc_id')\n",
    "    df = df.rename(columns={'text': 'section_text'})\n",
    "    df = df.sort_values(by='is_covid_related', ascending = False).reset_index(drop=True)\n",
    "    \n",
    "    df = df[df['publish_time'] > datetime(2019, 1, 1)]\n",
    "    \n",
    "    count_dict = df.groupby('doc_id').count()['is_covid_related'].to_dict()\n",
    "    df['count'] = df['doc_id'].apply(lambda x: count_dict[x])\n",
    "    df = df.sort_values('count', ascending=False)\n",
    "    df = df.drop('count', axis=1).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging dataframe with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering cov_risk_design\n",
    "cov_risk_design = pd.read_csv(downloads + 'cov_risk_design.csv',index_col = 0, keep_default_na=False)\n",
    "cov_risk_design_rich = join_metadata(cov_risk_design)\n",
    "cov_risk_design_rich.to_json(downloads + 'cov_risk_design.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_design = pd.read_csv(downloads + 'risk_design.csv', index_col=0, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_design_rich = join_metadata(risk_design)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering sections for whether they are covid-related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_design_rich = risk_design_rich[risk_design_rich['is_covid_related']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = [lemmatizer.lemmatize(section) for section \n",
    "            in list(risk_design_rich['section']) \n",
    "            if section != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency table for most common sections in covid related papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_secs = {}\n",
    "for section in sections:\n",
    "    if section in freq_secs:\n",
    "        freq_secs[section] += 1\n",
    "    else:\n",
    "        freq_secs[section] = 1\n",
    "\n",
    "sorted(freq_secs.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Filtering noise by removing sentences with hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df = pd.read_json(downloads + 'cov_risk_design_summarized.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_url(sent):\n",
    "    url_match = re.findall('https?://', sent)\n",
    "    return bool(url_match)\n",
    "\n",
    "\n",
    "def is_valid_sent(sent):\n",
    "    words = word_tokenize(sent)\n",
    "    words = [\"\".join(re.findall(\"[a-zA-Z]+\", word)) for word in words]\n",
    "    words = [word for word in words if len(word) > 3]\n",
    "    return len(words) > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bib(text):\n",
    "    raw_sents = sent_tokenize(text)\n",
    "    raw_sents = [sent for sent in raw_sents if is_valid_sent(sent)]\n",
    "    clean_sents = []\n",
    "    \n",
    "    length = len(raw_sents)\n",
    "    start = 0\n",
    "    end = 5 if  (start - 5 < length) else (length - start) \n",
    "    win = [is_url(sent) for sent in raw_sents[start:end]]\n",
    "        \n",
    "    while end < length:\n",
    "        if sum(win) > 0.5 * len(win):\n",
    "            temp = end\n",
    "            end += 5\n",
    "            start = temp + 1\n",
    "            win = [is_url(sent) for sent in raw_sents[start:end]]\n",
    "        else:\n",
    "            clean_sents.append(raw_sents[start])\n",
    "            start += 1\n",
    "            end += 1\n",
    "            win = win[1:]\n",
    "            try:\n",
    "                win.append(has_url(raw_sents[end]))\n",
    "            except:\n",
    "                print('ERROR:', length, startt, e)\n",
    "        \n",
    "    return \" \".join(clean_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['doc_id', 'covid19_in_text', 'risk_factor', 'section_text', 'section',\n",
       "       'design_study', 'is_covid_related', 'title', 'abstract', 'text_body',\n",
       "       'publish_time', 'authors', 'journal', 'doi', 'H index',\n",
       "       'scibert_summary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = cov_df.iloc[-3]['section_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 11, 11, 0, 2, 33, 0]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(re.findall(\"(http|doi|www)\", sec)) for sec in cov_df['section_text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org /10.1101 /10. /2020 pooled severe incidence and fatality rate is significantly lower compared with sars and mers, which may explain why the novel coronavirus has spread so widely 46 . of note, there are regional and spatial differences in the incidence rate of covid-19. in our research, the pooled severity rate and mortality caused by covid-19 was found significantly higher in wuhan than that of the infected outside of wuhan (all for p < 0.01). on the other hand, disease incidence at the early stage of outbreak was higher than that at the late stage, which may be caused by the lack of recognitions and treatment experience for covid-19. moreover, the longer time from symptoms to hospitalization, the higher incidence rate of the mortality related to covid-19, highlighting the importance of timely medical treatment 30 . in addition, among the patients with 2019-ncov, the pooled infection rate of medical staff was 7.7%, which awareness and others. nevertheless, we conducted meta-regression based on the observation duration and symptom onset to hospital admission time, which explained a large percent of heterogeneity. secondly, studies published before february 25, 2020 and articles published in english only were included in our study, therefore there was lack of data from other countries. however, our meta-analysis involved 53000 confirmed patients based on the data during the early-to-mid period of disease outbreak in china, which will provide great referential value for global epidemic control. thirdly, meta-analysis was conducted on the level of the studies and the characteristics of individual patients could not be retrieved, thus it was hard to provide reference for individualized diagnosis and treatment of covid-19. finally, all included studies were retrospective, as no randomized control trials and prospective studies related to 2019-ncov finish till now, thus our results require to be confirmed by more high-quality clinical researches.'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#risk_regrs = \"(\" + \"|\".join(risk_factors) + \")\"\n",
    "len(re.findall(\"staff\", sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- Show H Index of paper\n",
    "- Rank by number of \"http, doi or www\" (lower rank for more occurences)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv11568a5b709c405b925f37b6b0b6dbdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
